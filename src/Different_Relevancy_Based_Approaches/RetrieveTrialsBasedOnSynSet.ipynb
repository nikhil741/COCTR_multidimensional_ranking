{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from quickUmls.client import get_quickumls_client\n",
    "import pickle\n",
    "from os import mkdir\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSynWords(word):\n",
    "    similarWords = []\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            similarWords.append(lemma.name())\n",
    "    return list(set(similarWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"(?:^(?:never|no|nothing|nowhere|noone|none|not|havent|hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint)$)|n't\"\n",
    "endSymbolsTillNegation = [',', '.', ':', ';', '!', '?']\n",
    "#  ‘,’,\n",
    "# ‘.’, ‘:’, ‘;’, ‘!’, ‘?’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeSymbolsList = ['∆', '(', ')', ',', '.', 'β', 'α', \"'s'\", '$', '``', \"''\", \"'s\", ':', ';', '/', '\\\\', '+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = get_quickumls_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query_path = \"../data/githubAnnotatedDataAndIncExcAppended\"\n",
    "output_path = \"../data/synonymsWordsFieldWiseFrequency\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dietary Therapy Epilepsies_page_rank.csv',\n",
       " 'Early Parkinson disease treatment_page_rank.csv',\n",
       " 'HIV infection Treatment naive_page_rank.csv',\n",
       " 'HIV infection seronegativity_page_rank.csv',\n",
       " 'Hypercholesterolemia safe treatments_page_rank.csv',\n",
       " 'Nonvalvular atrial fibrillation_page_rank.csv',\n",
       " 'Outcomes of cerebrovascular accident_page_rank.csv',\n",
       " 'Treating Anemia, Iron-Deficiency in CKD patients_page_rank_15.csv',\n",
       " 'already having Celiac Disease_page_rank.csv',\n",
       " 'antiretroviral therapy first time_pageRank.csv',\n",
       " 'constipation safe treatments_page_rank.csv',\n",
       " 'dietary approaches for obesity treatment_page_rank.csv',\n",
       " 'haemorrhage cure_page_rank.csv',\n",
       " 'hypertension safe treatments_page_rank.csv',\n",
       " 'low back pain therapy workout_pageRank.csv',\n",
       " 'malnutrition in young children_page_rank.csv',\n",
       " 'managing constipation in children_pageRank.csv',\n",
       " 'out of hospital cardiac arrest_page_rank.csv',\n",
       " 'postoperative delirium_pageRank.csv',\n",
       " 'recommended anti-platelet doses for treating Coronary artery disease_page_rank_103.csv',\n",
       " 'safe treatment for Alzheimer disease_page_rank.csv',\n",
       " 'safe treatments for asthma_pageRank.csv',\n",
       " 'serious Rheumatoid arthritis_pageRank.csv',\n",
       " 'serious sleep apnea_page_rank.csv',\n",
       " 'treating people already having hypertension_page_rank.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = sorted(listdir(input_query_path))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfWordsForWhichUMLSConceptdidntGen(query):\n",
    "    final_query = query\n",
    "    concepts = matcher.match(query, best_match=True, ignore_syntax=False)\n",
    "    conceptOfQuery = [elem[0][u'cui'] for elem in concepts]\n",
    "#     print(conceptOfQuery)\n",
    "    for concept in concepts:\n",
    "        toRemove = \"\"\n",
    "        toRemove += query[concept[0]['start']:concept[0]['end']]\n",
    "        final_query = final_query.replace(toRemove, \"\")\n",
    "        final_query = final_query.replace(\"  \", \" \")\n",
    "#     print(final_query)\n",
    "    \n",
    "    final_query = final_query.split()\n",
    "#     final_query = [ps.stem(word) for word in final_query]\n",
    "#     final_query = [lemmatizer.lemmatize(word) for word in final_query]\n",
    "    final_query = [word for word in final_query if word not in stopWords]\n",
    "    final_query = [word.lower() for word in final_query]\n",
    "    return final_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Brieftitle\n",
    "def briefTitle(df, queryWordsToSearch):\n",
    "    matchedCountList = []\n",
    "    df['brief_title'] = df['brief_title'].fillna(\"\")\n",
    "    for title in df['brief_title']:\n",
    "        matched = 0\n",
    "#         print(title)\n",
    "        tokenizedWords = word_tokenize(title)\n",
    "        newL = []\n",
    "        for word in tokenizedWords:\n",
    "            if '-' in word:\n",
    "                newL += word.split('-')\n",
    "                \n",
    "            else:\n",
    "                newL.append(word)\n",
    "        \n",
    "        tokenizedWords = newL\n",
    "        tokenizedWords = [ps.stem(word) for word in tokenizedWords]\n",
    "        tokenizedWords = [word for word in tokenizedWords if word not in stopWords]\n",
    "        tokenizedWords = [word for word in tokenizedWords if word not in removeSymbolsList]\n",
    "        tokenizedWords = [word.lower() for word in tokenizedWords]\n",
    "#         print(tokenizedWords)\n",
    "        \n",
    "        for queryWord in queryWordsToSearch:\n",
    "            if queryWord in tokenizedWords:\n",
    "                matched += 1\n",
    "        matchedCountList.append(matched)\n",
    "#     print(matchedCountList)\n",
    "    return matchedCountList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Official Title\n",
    "def officialTitle(df, queryWordsToSearch):\n",
    "    matchedCountList = []\n",
    "    df['official_title'] = df['official_title'].fillna(\"\")\n",
    "    for title in df['official_title']:\n",
    "        matched = 0\n",
    "#         print(title)\n",
    "        tokenizedWords = word_tokenize(title)\n",
    "        newL = []\n",
    "        for word in tokenizedWords:\n",
    "            if '-' in word:\n",
    "                newL += word.split('-')\n",
    "                \n",
    "            else:\n",
    "                newL.append(word)\n",
    "        \n",
    "        tokenizedWords = newL\n",
    "        tokenizedWords = [ps.stem(word) for word in tokenizedWords]\n",
    "        tokenizedWords = [word for word in tokenizedWords if word not in stopWords]\n",
    "        tokenizedWords = [word for word in tokenizedWords if word not in removeSymbolsList]\n",
    "        tokenizedWords = [word.lower() for word in tokenizedWords]\n",
    "#         print(tokenizedWords)\n",
    "        \n",
    "        for queryWord in queryWordsToSearch:\n",
    "            if queryWord in tokenizedWords:\n",
    "                matched += 1\n",
    "        matchedCountList.append(matched)\n",
    "#     print(matchedCountList)\n",
    "    return matchedCountList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conditions List\n",
    "def conditionsList(df, queryWordsToSearch):\n",
    "    matchedCountList = []\n",
    "    df['conditions_list'] = df['conditions_list'].fillna(\"\")\n",
    "    for title in df['conditions_list']:\n",
    "        matched = 0\n",
    "#         print(title)\n",
    "        tokenizedWords = title.split(';')\n",
    "        newL = []\n",
    "        for word in tokenizedWords:\n",
    "            newL += word_tokenize(word)\n",
    "        tokenizedWords = newL\n",
    "        tokenizedWords = [ps.stem(word) for word in tokenizedWords]\n",
    "        tokenizedWords = [word for word in tokenizedWords if word not in stopWords]\n",
    "        tokenizedWords = [word for word in tokenizedWords if word not in removeSymbolsList]\n",
    "        tokenizedWords = [word.lower() for word in tokenizedWords]\n",
    "#         print(tokenizedWords)\n",
    "        \n",
    "        for queryWord in queryWordsToSearch:\n",
    "            if queryWord in tokenizedWords:\n",
    "                matched += 1\n",
    "        matchedCountList.append(matched)\n",
    "#     print(matchedCountList)\n",
    "    return matchedCountList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keywords List\n",
    "def keywordsList(df, queryWordsToSearch):\n",
    "    matchedCountList = []\n",
    "    df['keywords_list'] = df['keywords_list'].fillna(\"\")\n",
    "    for title in df['keywords_list']:\n",
    "        matched = 0\n",
    "#         print(title)\n",
    "        tokenizedWords = title.split(';')\n",
    "        newL = []\n",
    "        for word in tokenizedWords:\n",
    "            newL += word_tokenize(word)\n",
    "        tokenizedWords = newL\n",
    "        tokenizedWords = [ps.stem(word) for word in tokenizedWords]\n",
    "        tokenizedWords = [word for word in tokenizedWords if word not in stopWords]\n",
    "        tokenizedWords = [word for word in tokenizedWords if word not in removeSymbolsList]\n",
    "        tokenizedWords = [word.lower() for word in tokenizedWords]\n",
    "#         print(tokenizedWords)\n",
    "        \n",
    "        for queryWord in queryWordsToSearch:\n",
    "            if queryWord in tokenizedWords:\n",
    "                matched += 1\n",
    "        matchedCountList.append(matched)\n",
    "#     print(matchedCountList)\n",
    "    return matchedCountList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Brief Summary\n",
    "def briefSummary(df, queryWordsToSearch):\n",
    "    row = 2\n",
    "    matchedCountList = []\n",
    "    df['description'] = df['description'].fillna(\"\")\n",
    "    for title in df['description']:\n",
    "        matched = 0\n",
    "#         print(title)\n",
    "        tokenizedWords = word_tokenize(title)\n",
    "        newL = []\n",
    "        for word in tokenizedWords:\n",
    "            if '-' in word:\n",
    "                newL += word.split('-')\n",
    "                \n",
    "            else:\n",
    "                newL.append(word)\n",
    "        \n",
    "        tokenizedWords = newL\n",
    "        tokenizedWords = [ps.stem(word) for word in tokenizedWords]\n",
    "        tokenizedWords = [word for word in tokenizedWords if word not in stopWords]\n",
    "        tokenizedWords = [word for word in tokenizedWords if word not in removeSymbolsList]\n",
    "        tokenizedWords = [word.lower() for word in tokenizedWords]\n",
    "#         print(row)\n",
    "#         print(tokenizedWords)\n",
    "#         print()\n",
    "        \n",
    "        for queryWord in queryWordsToSearch:\n",
    "            if queryWord in tokenizedWords:\n",
    "                matched += 1\n",
    "        matchedCountList.append(matched)\n",
    "        row += 1\n",
    "#     print(matchedCountList)\n",
    "    return matchedCountList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processEachInclusionRow(inc_sentences):\n",
    "#     matched = 0\n",
    "    bigWordListForAllSentences = []\n",
    "\n",
    "    for sentence in inc_sentences.split(';;;;;;;;;;'):\n",
    "        sentence = sentence.replace(\"             \", \" \")\n",
    "        sentence = word_tokenize(sentence)\n",
    "#             print(sentence)\n",
    "        bigWordListForAllSentences += sentence\n",
    "        bigWordListForAllSentences.append(\".\")\n",
    "    \n",
    "    bigWordListForAllSentences = [word.lower() for word in bigWordListForAllSentences]\n",
    "    \n",
    "    bigWordNegationFlagList = [0]*len(bigWordListForAllSentences)\n",
    "    \n",
    "    \n",
    "    wordIndex = 0\n",
    "    while wordIndex < len(bigWordNegationFlagList):\n",
    "        word = bigWordListForAllSentences[wordIndex]\n",
    "        regMatch = re.search(pattern, word)\n",
    "        if regMatch:\n",
    "#             print(\"TRUE\", word)\n",
    "            while wordIndex < len(bigWordNegationFlagList) and bigWordListForAllSentences[wordIndex] not in endSymbolsTillNegation:\n",
    "                bigWordNegationFlagList[wordIndex] = 1\n",
    "                wordIndex += 1\n",
    "        wordIndex += 1\n",
    "    \n",
    "    \n",
    "    bigWordListForAllSentences = [ps.stem(word) for word in bigWordListForAllSentences]\n",
    "#     print(\"BigList:\", bigWordListForAllSentences)\n",
    "#     print(bigWordNegationFlagList)\n",
    "    \n",
    "    return bigWordListForAllSentences, bigWordNegationFlagList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWordInInclusionContext(queryWordsToSearch, bigWordListForAllSentences, bigWordNegationFlagList):\n",
    "    matchedCountPos = 0\n",
    "    matchedCountNeg = 0\n",
    "    \n",
    "    for query_word in queryWordsToSearch:\n",
    "        for sent_word_neg_flag_tup in zip(bigWordListForAllSentences, bigWordNegationFlagList):\n",
    "            sent_word = sent_word_neg_flag_tup[0]\n",
    "            sent_word_context_flag = sent_word_neg_flag_tup[1]\n",
    "            \n",
    "            if query_word == sent_word:\n",
    "                if sent_word_context_flag == 0:\n",
    "                    matchedCountPos += 1\n",
    "                else:\n",
    "                    matchedCountNeg += 1\n",
    "#                 print(\"Matched:\", \"Query:Word\", query_word, \"SentenceWord:\", sent_word, \"Context:\", sent_word_context_flag)\n",
    "    return matchedCountPos, matchedCountNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inclusionCriteria(df, queryWordsToSearch):\n",
    "    matchedCountListPos = []\n",
    "    matchedCountListNeg = []\n",
    "    df['inclusion'] = df['inclusion'].fillna(\"\")\n",
    "    for inc_sentences in df['inclusion']:\n",
    "        bigWordListForAllSentences, bigWordNegationFlagList = processEachInclusionRow(inc_sentences)\n",
    "        matchedCountPos, matchedCountNeg = findWordInInclusionContext(queryWordsToSearch, bigWordListForAllSentences, bigWordNegationFlagList)\n",
    "        matchedCountListPos.append(matchedCountPos)\n",
    "        matchedCountListNeg.append(matchedCountNeg)\n",
    "    return matchedCountListPos, matchedCountListNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processEachExclusionRow(exc_sentences):\n",
    "#     matched = 0\n",
    "    bigWordListForAllSentences = []\n",
    "\n",
    "    for sentence in exc_sentences.split(';;;;;;;;;;'):\n",
    "        sentence = sentence.replace(\"             \", \" \")\n",
    "        sentence = word_tokenize(sentence)\n",
    "#             print(sentence)\n",
    "        bigWordListForAllSentences += sentence\n",
    "        bigWordListForAllSentences.append(\".\")\n",
    "    \n",
    "    bigWordListForAllSentences = [word.lower() for word in bigWordListForAllSentences]\n",
    "    \n",
    "    bigWordNegationFlagList = [0]*len(bigWordListForAllSentences)\n",
    "    \n",
    "    \n",
    "    wordIndex = 0\n",
    "    while wordIndex < len(bigWordNegationFlagList):\n",
    "        word = bigWordListForAllSentences[wordIndex]\n",
    "        regMatch = re.search(pattern, word)\n",
    "        if regMatch:\n",
    "#             print(\"TRUE\", word)\n",
    "            while wordIndex < len(bigWordNegationFlagList) and bigWordListForAllSentences[wordIndex] not in endSymbolsTillNegation:\n",
    "                bigWordNegationFlagList[wordIndex] = 1\n",
    "                wordIndex += 1\n",
    "        wordIndex += 1\n",
    "    \n",
    "    \n",
    "    bigWordListForAllSentences = [ps.stem(word) for word in bigWordListForAllSentences]\n",
    "#     print(\"BigList:\", bigWordListForAllSentences)\n",
    "#     print(bigWordNegationFlagList)\n",
    "    \n",
    "    return bigWordListForAllSentences, bigWordNegationFlagList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWordInExclusionContext(queryWordsToSearch, bigWordListForAllSentences, bigWordNegationFlagList):\n",
    "    matchedCountPos = 0\n",
    "    matchedCountNeg = 0\n",
    "    \n",
    "    for query_word in queryWordsToSearch:\n",
    "        for sent_word_neg_flag_tup in zip(bigWordListForAllSentences, bigWordNegationFlagList):\n",
    "            sent_word = sent_word_neg_flag_tup[0]\n",
    "            sent_word_context_flag = sent_word_neg_flag_tup[1]\n",
    "            \n",
    "            if query_word == sent_word:\n",
    "                if sent_word_context_flag == 0:\n",
    "                    matchedCountPos += 1\n",
    "                else:\n",
    "                    matchedCountNeg += 1\n",
    "#                 print(\"Matched:\", \"Query:Word\", query_word, \"SentenceWord:\", sent_word, \"Context:\", sent_word_context_flag)\n",
    "    return matchedCountPos, matchedCountNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclusionCriteria(df, queryWordsToSearch):\n",
    "    matchedCountListPos = []\n",
    "    matchedCountListNeg = []\n",
    "    df['exclusion'] = df['exclusion'].fillna(\"\")\n",
    "    for inc_sentences in df['exclusion']:\n",
    "        bigWordListForAllSentences, bigWordNegationFlagList = processEachInclusionRow(inc_sentences)\n",
    "        matchedCountPos, matchedCountNeg = findWordInExclusionContext(queryWordsToSearch, bigWordListForAllSentences, bigWordNegationFlagList)\n",
    "        matchedCountListPos.append(matchedCountPos)\n",
    "        matchedCountListNeg.append(matchedCountNeg)\n",
    "    return matchedCountListPos, matchedCountListNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dietary Therapy Epilepsies\n",
      "['dietet', 'dietet', 'dietari']\n",
      "Early Parkinson disease treatment\n",
      "['early_on', 'former', 'too_soon', 'ahead_of_tim', 'betim', 'earli']\n",
      "HIV infection Treatment naive\n",
      "['naif', 'naiv', 'primit', 'unenlighten', 'uniniti', 'uninstruct', 'uniniti']\n",
      "HIV infection seronegativity\n",
      "['seroneg']\n",
      "Hypercholesterolemia safe treatments\n",
      "['secur', 'condom', 'depend', 'prophylact', 'good', 'safeti', 'safe', 'rubber']\n",
      "Nonvalvular atrial fibrillation\n",
      "['nonvalvular']\n",
      "Outcomes of cerebrovascular accident\n",
      "['result', 'result', 'consequ', 'upshot', 'termin', 'outcom', 'final_result', 'effect', 'outcom', 'event', 'issu']\n",
      "Treating Anemia, Iron-Deficiency in CKD patients\n",
      "['do_bi', 'care_for', 'address', 'process', 'affected_rol', 'plow', 'treat', 'patient_rol', 'patient', 'ckd', 'treat', 'regal', 'handl', 'cover', 'deal', 'patient']\n",
      "already having Celiac Disease\n",
      "['alreadi']\n",
      "antiretroviral therapy first time\n",
      "['first_of_al', 'first-class_honours_degre', 'first_off', 'maiden', 'clock', 'clock_tim', 'low', 'inaugur', '1st', 'number_on', 'time', 'initiatori', 'initi', 'foremost', 'kickoff', 'prison_term', 'metr', 'clip', 'outset', 'first_bas', 'low_gear', 'commenc', 'starting_tim', 'first', 'for_the_first_tim', 'fourth_dimens', 'first_gear', 'firstli', 'sentenc', 'meter', 'showtim', 'world-class', 'get-go', 'number_1', 'offset', 'begin', 'start']\n",
      "constipation safe treatments\n",
      "['secur', 'condom', 'depend', 'prophylact', 'good', 'safeti', 'safe', 'rubber']\n",
      "dietary approaches for obesity treatment\n",
      "['feeler', 'near', 'plan_of_attack', 'approach_path', 'dietet', 'attack', 'come', 'glide_path', 'approach', 'draw_clos', 'approach', 'glide_slop', 'border_on', 'go_about', 'dietet', 'approach', 'dietari', 'access', 'draw_near', 'go_up', 'set_about', 'come_near', 'overtur', 'advanc', 'come_on', 'approach_shot']\n",
      "haemorrhage cure\n",
      "['therapeut', 'remedi', 'cure', 'bring_around', 'heal', 'cur']\n",
      "hypertension safe treatments\n",
      "['secur', 'condom', 'depend', 'prophylact', 'good', 'safeti', 'safe', 'rubber']\n",
      "low back pain therapy workout\n",
      "['exercis', 'physical_exert', 'physical_exercis', 'workout', 'exercis']\n",
      "malnutrition in young children\n",
      "['youth', 'kid', 'lester_willis_young', 'vernal', 'cy_young', 'loretta_young', 'fri', 'young', 'brigham_young', 'thomas_young', 'new', 'minor', 'tyke', 'untest', 'child', 'tike', 'young', 'children', 'tiddler', 'youngster', 'danton_true_young', 'immatur', 'small_fri', 'pres_young', 'offspr', 'youth', 'whitney_moore_young_jr.', 'babi', 'shaver', 'nipper', 'unseason', 'edward_young', 'untri', 'nestl', 'whitney_young']\n",
      "managing constipation in children\n",
      "['bring_off', 'kid', 'superintend', 'finagl', 'get_bi', 'fri', 'make_out', 'grappl', 'minor', 'tyke', 'overse', 'pull_off', 'supervis', 'deal', 'child', 'carry_off', 'tike', 'wangl', 'handl', 'make_do', 'children', 'tiddler', 'youngster', 'small_fri', 'wield', 'contend', 'negoci', 'babi', 'manag', 'shaver', 'nipper', 'nestl', 'cope', 'care', 'manag']\n",
      "out of hospital cardiac arrest\n",
      "['infirmari', 'hospit']\n",
      "postoperative delirium\n",
      "['postop']\n",
      "recommended anti-platelet doses for treating Coronary artery disease\n",
      "['department_of_st', 'address', \"cupid's_itch\", 'acid', 'plow', 'treat', 'social_diseas', 'do', 'treat', 'united_states_department_of_st', 'regal', 'superman', 'cover', 'do_bi', 'commend', 'venereal_infect', 'urg', 'process', 'battery-acid', 'advoc', 'pane', 'recommend', 'zen', 'deal', 'recommend', 'sexually_transmitted_diseas', 'do', 'window_pan', 'loony_toon', 'state_depart', 'VD', \"venus's_curs\", 'dose', 'anti-platelet', 'handl', \"cupid's_diseas\", 'state', 'disk_operating_system', 'care_for', 'drug', 'std', 'elvi', 'dot', 'lucy_in_the_sky_with_diamond', 'dosag', 'venereal_diseas', 'dose', 'back_break']\n",
      "safe treatment for Alzheimer disease\n",
      "['secur', 'condom', 'depend', 'prophylact', 'good', 'safeti', 'safe', 'rubber']\n",
      "safe treatments for asthma\n",
      "['secur', 'condom', 'depend', 'prophylact', 'good', 'safeti', 'safe', 'rubber']\n",
      "serious Rheumatoid arthritis\n",
      "['danger', 'grave', 'unplay', 'sever', 'seriou', 'sober', 'good', 'life-threaten', 'grievou']\n",
      "serious sleep apnea\n",
      "['danger', 'grave', 'unplay', 'sever', 'seriou', 'sober', 'good', 'life-threaten', 'grievou']\n",
      "treating people already having hypertension\n",
      "['address', 'peopl', 'hoi_polloi', 'plow', 'treat', 'treat', 'regal', 'cover', 'do_bi', 'process', 'alreadi', 'deal', 'mass', 'handl', 'care_for', 'citizenri', 'the_great_unwash', 'multitud', 'mass']\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    query = file.split(\"_\")[0]\n",
    "    print(query)\n",
    "    queryWordsToSearch = getListOfWordsForWhichUMLSConceptdidntGen(query)\n",
    "#     print(queryWordsToSearch)\n",
    "    newQueryList = queryWordsToSearch.copy()\n",
    "    for word in queryWordsToSearch:\n",
    "        newQueryList += getSynWords(word)\n",
    "#         print(word, \"Hello\")\n",
    "    queryWordsToSearch = list(set(newQueryList))\n",
    "    queryWordsToSearch = [word for word in queryWordsToSearch if word not in stopWords]\n",
    "    queryWordsToSearch = [ps.stem(word) for word in queryWordsToSearch]\n",
    "    print(queryWordsToSearch)\n",
    "    \n",
    "    df = pd.read_csv(join(input_query_path, file))\n",
    "#     print(df.columns)\n",
    "    \n",
    "    matchList = briefTitle(df, queryWordsToSearch)\n",
    "    df['brief_title_matched_count'] = matchList\n",
    "    \n",
    "    matchList = officialTitle(df, queryWordsToSearch)\n",
    "    df['official_title_matched_count'] = matchList\n",
    "    \n",
    "    matchList = conditionsList(df, queryWordsToSearch)\n",
    "    df['conditions_list_matched_count'] = matchList\n",
    "    \n",
    "    matchList = keywordsList(df, queryWordsToSearch)\n",
    "    df['keywords_list_matched_count'] = matchList\n",
    "    \n",
    "#     print(keywordsList(df, queryWordsToSearch))\n",
    "    matchList = briefSummary(df, queryWordsToSearch)\n",
    "    df['brief_summary_matched_count'] = matchList\n",
    "    \n",
    "    matchedCountListPos, matchedCountListNeg = inclusionCriteria(df, queryWordsToSearch)\n",
    "    df['inclusion_pos'] = matchedCountListPos\n",
    "    df['inclusion_neg'] = matchedCountListNeg\n",
    "    \n",
    "    matchedCountListPos, matchedCountListNeg = exclusionCriteria(df, queryWordsToSearch)\n",
    "    df['exclusion_pos'] = matchedCountListPos\n",
    "    df['exclusion_neg'] = matchedCountListNeg\n",
    "    \n",
    "    df.to_csv(join(output_path, file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['most', 'well-nigh', 'nearly', 'about', 'virtually', 'near', 'almost', 'nigh']\n"
     ]
    }
   ],
   "source": [
    "print(getSynWords(\"almost\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
